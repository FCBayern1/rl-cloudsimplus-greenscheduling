# ===================================================================
# config.yml - Default Configuration for DRL Load Balancing/Scaling
# ===================================================================

common:
  # --- Simulation Identification ---
  simulation_name: "DefaultSimRun" # Used in logging and result file names

  # --- Simulation Control & Timing ---
  simulation_timestep: 1.0 # Duration of each RL agent step (seconds)
  min_time_between_events: 0.1 # CloudSim internal minimum time granularity (s)
  max_episode_length: 500 # Max steps per episode before truncation

  # --- Host Configuration ---
  hosts_count: 32 # Number of physical hosts/servers
  host_pes: 16 # Processing Elements (cores) per host
  host_pe_mips: 50000 # MIPS capacity per PE (core)
  host_ram: 65536 # Host RAM in MB (64 GB)
  host_bw: 50000 # Host Bandwidth in Mbps (50 Gbps)
  host_storage: 100000 # Host Storage in MB (100 GB)

  # --- VM Configuration (Base 'S' & Multipliers) ---
  small_vm_pes: 2 # PEs for base 'S' VM (like m5a.large)
  small_vm_ram: 8192 # RAM (MB) for 'S' VM (8 GB)
  small_vm_bw: 1000 # Bandwidth (Mbps) for 'S' VM (1 Gbps)
  small_vm_storage: 4000 # Storage (MB) for 'S' VM (4 GB)
  medium_vm_multiplier: 2 # 'M' VM PEs/RAM = multiplier * 'S' (e.g., 4 PEs, 16GB RAM)
  large_vm_multiplier: 4 # 'L' VM PEs/RAM = multiplier * 'S' (e.g., 8 PEs, 32GB RAM)

  # --- Initial VM Fleet ---
  initial_s_vm_count: 20 # Start with 0 Small VMs
  initial_m_vm_count: 10 # Start with 0 Medium VM
  initial_l_vm_count: 5 # Start with 0 Large VMs

  # --- VM Lifecycle Delays ---
  vm_startup_delay: 0.0 # Time (sec) for VM to boot (reduced for faster testing)
  vm_shutdown_delay: 0.0 # Time (sec) before idle VM is destroyed by broker (agent action is immediate)

  # --- Workload Configuration ---
  workload_mode: "SWF" # Options: "SWF", "CSV"
  cloudlet_trace_file: "traces/LLNL-Atlas-2006-2.1-cln-test.swf" # Default trace file path (relative to resources)
  max_cloudlets_to_create_from_workload_file: 2147483647 # Limit lines read for faster testing (Set to MAX_INT for full file)
  workload_reader_mips: 50000 # MIPS ref for SWF length calc (match host_pe_mips)
  split_large_cloudlets: true # Split cloudlets requesting more PEs than max_cloudlet_pes?
  max_cloudlet_pes: 8 # Max PEs a single (split) cloudlet can request (e.g., = large VM PEs)

  # --- Costing Configuration ---
  small_vm_hourly_cost: 0.086 # Approx AWS m5a.large hourly cost ($)
  paying_for_the_full_hour: false # Billing model (false = per-second approx)

  # --- Reward Weights ---
  # Note: Tune these weights heavily based on experimental results!
  reward_wait_time_coef: 0.75 # Penalty for FINISHED CLOUDLETS WAIT TIME (Avg per step)
  reward_throughput_coef: 0.85 # Reward for THROUGHPUT (Avg Cloudlets Finished per step)
  reward_unutilization_coef: 0.25 # Penalty for avg VM CPU UNUTILIZATION (1.0 - utilization)
  reward_cost_coef: 0.35 # Penalty for INFRASTRUCTURE COST (Allocated Cores / Total Cores Ratio)
  reward_queue_penalty_coef: 0.55 # Penalty for number of WAITING cloudlets (normalized)
  reward_invalid_action_coef: 1.0 # Flat penalty for attempting an invalid action

  # --- Observation Normalization ---
  max_queue_norm: 100.0 # Expected max queue length for normalizing observation
  max_pes_norm: 8.0 # Expected max PEs for normalizing next_cloudlet_pes obs (e.g., largest VM size)
  # tree_array_max_len: 5000          # Optional: Manually set Tree Array padding size (if calculated value is unstable/undesired)

  # --- Technical Flags ---
  clear_created_lists: true # Optimization: Clear internal CloudSim broker lists

  # --- Python/RL Parameters ---
  env_id: "LoadBalancingScaling-v0" # Gym environment ID to use
  mode: "train" # Default execution mode: "train", "test"
  algorithm: "PPO" # RL algorithm ("MaskablePPO", "PPO", "A2C", etc.)
  policy: "MultiInputPolicy" # Network policy for Dict observation space
  timesteps: 1000 # Total training timesteps
  learning_rate: 0.0003 # Optimizer learning rate
  n_steps: 2048 # (PPO/A2C) Steps per policy update
  batch_size: 64 # (PPO/Others) Minibatch size for training
  n_epochs: 10 # (PPO) Optimisation epochs per update
  gamma: 0.99 # Discount factor for future rewards
  gae_lambda: 0.95 # (PPO/A2C) Factor for Generalized Advantage Estimation
  clip_range: 0.2 # (PPO) Clipping parameter
  ent_coef: 0.01 # Entropy coefficient (exploration bonus)
  vf_coef: 0.5 # Value function coefficient in loss
  max_grad_norm: 0.5 # Max gradient norm for clipping
  # policy_kwargs: # Optional: e.g., net_arch: [128, 128]
  verbose: 0 # SB3 Verbosity level (0=minimal, 1=info, 2=debug)
  log_interval: 1 # Log stats every N episodes
  seed: 4567 # Default random seed for Python/Torch/Numpy/Env
  save_experiment: true # Save logs, models, etc.
  base_log_dir: "logs" # Root directory for all logs
  experiment_type_dir: "DefaultType" # Subdirectory for this type of experiment (e.g., "SWF_Tests", "CSV_Tests")
  train_model_dir: "" # REQUIRED FOR TEST/TRANSFER: Path to trained model log dir (relative to base_log_dir)
  save_replay_buffer: false # Save replay buffer with best model (only for off-policy algos)
  device: "auto" # Device for PyTorch ("auto", "cpu", "cuda")

# ===================================================================
# Experiment-Specific Overrides (Define different runs below)
# ===================================================================

experiment_1:
  simulation_name: "Exp1_CSVSimple_Ent_0_01" # Unique name for logging this specific run
  experiment_name: "Exp1_CSVSimple_Ent_0_01" # Name for the log sub-folder
  experiment_type_dir: "CSV_Train"
  workload_mode: "CSV"
  cloudlet_trace_file: "traces/three_60max_8maxcores.csv"

# ===================================================================
# Experiment 2: Large-Scale LLNL Workload with Optimized Parameters
# ===================================================================
experiment_2:
  simulation_name: "Exp2_LLNL_Large_Optimized"
  experiment_name: "exp2_llnl_large"
  experiment_type_dir: "LargeScale_Experiments"
  
  # Workload Configuration
  workload_mode: "SWF"
  cloudlet_trace_file: "traces/LLNL-Atlas-2006-2.1-cln.swf"
  max_cloudlets_to_create_from_workload_file: 500  # Start with 500 tasks (full dataset: 43810)
  workload_reader_mips: 50000
  split_large_cloudlets: true
  max_cloudlet_pes: 8
  
  # RL Algorithm Configuration (Optimized)
  mode: "train"
  algorithm: "MaskablePPO"  # Use MaskablePPO to reduce invalid actions
  timesteps: 50000  # Longer training
  learning_rate: 0.0001  # Lower LR for stability
  
  # PPO Hyperparameters
  n_steps: 512  # Smaller for more frequent updates
  batch_size: 128
  n_epochs: 15
  gamma: 0.995
  gae_lambda: 0.98
  clip_range: 0.2
  ent_coef: 0.02  # Increase exploration
  
  # Infrastructure Configuration (Optimized)
  hosts_count: 32
  host_pes: 16
  host_pe_mips: 50000
  host_ram: 65536
  
  # Reduce initial VMs to force RL to learn efficient scaling
  initial_s_vm_count: 10  # Reduced from 20
  initial_m_vm_count: 5   # Reduced from 10
  initial_l_vm_count: 2   # Reduced from 5
  
  # Reward Weights (Optimized)
  reward_wait_time_coef: 1.0        # Increase wait time penalty
  reward_throughput_coef: 0.85
  reward_unutilization_coef: 0.5    # Double utilization weight (from 0.25)
  reward_cost_coef: 0.5             # Increase cost weight (from 0.35)
  reward_queue_penalty_coef: 0.55
  reward_invalid_action_coef: 2.0   # Harshly penalize invalid actions (from 1.0)
  
  # Experiment Control
  max_episode_length: 500
  seed: 42  # Fixed seed for reproducibility
  save_experiment: true
  device: "auto"
  verbose: 1  # Show detailed training info
  log_interval: 1

# ===================================================================
# Experiment 3: Quick Test with CSV Workload (Fixed)
# ===================================================================
experiment_3:
  simulation_name: "Exp3_CSV_QuickTest_Fixed"
  experiment_name: "exp3_csv_quick"
  experiment_type_dir: "QuickTests"
  
  # Quick Test Configuration - Using CSV instead of SWF
  workload_mode: "CSV"
  cloudlet_trace_file: "traces/three_60max_8maxcores.csv"  # CSV with reasonable arrival times
  
  mode: "train"
  algorithm: "MaskablePPO"
  timesteps: 10000  # Increase for better learning
  
  # Use optimized parameters
  initial_s_vm_count: 10
  initial_m_vm_count: 5
  initial_l_vm_count: 2
  
  reward_unutilization_coef: 0.5
  reward_cost_coef: 0.5
  reward_invalid_action_coef: 2.0
  
  seed: 42
  save_experiment: true
  max_episode_length: 100  # Shorter for CSV workload (165 tasks)
  
# ===================================================================
# Experiment 4: Medium-Scale with Better Workload
# ===================================================================
experiment_4:
  simulation_name: "Exp4_MultiCSV_Optimized"
  experiment_name: "exp4_multi_csv"
  experiment_type_dir: "CSV_Optimized"
  
  # Use CSV workload for predictable arrival times
  workload_mode: "CSV"
  cloudlet_trace_file: "traces/three_60max_8maxcores.csv"
  
  # Optimized RL configuration
  mode: "train"
  algorithm: "MaskablePPO"
  timesteps: 50000
  learning_rate: 0.0001
  
  # PPO parameters
  n_steps: 512
  batch_size: 128
  n_epochs: 15
  gamma: 0.995
  gae_lambda: 0.98
  ent_coef: 0.02
  
  # Optimized infrastructure
  initial_s_vm_count: 10
  initial_m_vm_count: 5
  initial_l_vm_count: 2
  
  # Optimized rewards
  reward_wait_time_coef: 1.0
  reward_unutilization_coef: 0.5
  reward_cost_coef: 0.5
  reward_invalid_action_coef: 2.0
  
  max_episode_length: 100
  seed: 42
  save_experiment: true
  verbose: 1

# ===================================================================
# Experiment 5: Poisson Workload - Small Scale (112 tasks)
# ===================================================================
experiment_5:
  simulation_name: "Exp5_Poisson_Dense"
  experiment_name: "exp5_poisson_dense"
  experiment_type_dir: "Synthetic_Workloads"
  
  workload_mode: "CSV"
  cloudlet_trace_file: "traces/synthetic_dense_200s.csv"  # 396 tasks, dense arrivals (2.0/s)
  
  mode: "train"
  algorithm: "MaskablePPO"
  timesteps: 50000
  learning_rate: 0.0001
  
  # Optimized configuration
  initial_s_vm_count: 12  # More VMs for higher load
  initial_m_vm_count: 6
  initial_l_vm_count: 3
  
  reward_wait_time_coef: 1.0
  reward_unutilization_coef: 0.5
  reward_cost_coef: 0.5
  reward_invalid_action_coef: 2.0
  
  max_episode_length: 400  # Enough for 200s workload + execution time
  seed: 42
  save_experiment: true
  verbose: 1

# ===================================================================
# Experiment 6: Uniform Workload - Medium Scale (150 tasks)
# ===================================================================
experiment_6:
  simulation_name: "Exp6_Uniform_Medium"
  experiment_name: "exp6_uniform_medium"
  experiment_type_dir: "Synthetic_Workloads"
  
  workload_mode: "CSV"
  cloudlet_trace_file: "traces/synthetic_uniform_300s.csv"  # 150 tasks, 300s
  
  mode: "train"
  algorithm: "MaskablePPO"
  timesteps: 30000
  learning_rate: 0.0001
  
  n_steps: 512
  batch_size: 128
  n_epochs: 15
  
  initial_s_vm_count: 10
  initial_m_vm_count: 5
  initial_l_vm_count: 2
  
  reward_wait_time_coef: 1.0
  reward_unutilization_coef: 0.5
  reward_cost_coef: 0.5
  reward_invalid_action_coef: 2.0
  
  max_episode_length: 320
  seed: 42
  save_experiment: true
  verbose: 1

# ===================================================================
# Experiment 7: Large Scale Poisson (605 tasks) - RECOMMENDED
# ===================================================================
experiment_7:
  simulation_name: "Exp7_Poisson_Large"
  experiment_name: "exp7_poisson_large"
  experiment_type_dir: "Synthetic_Workloads"
  
  workload_mode: "CSV"
  cloudlet_trace_file: "traces/synthetic_large_600s.csv"  # 605 tasks, 600s
  
  mode: "train"
  algorithm: "MaskablePPO"
  timesteps: 100000  # Longer training for complex workload
  learning_rate: 0.0001
  
  # PPO hyperparameters
  n_steps: 512
  batch_size: 128
  n_epochs: 15
  gamma: 0.995
  gae_lambda: 0.98
  clip_range: 0.2
  ent_coef: 0.02
  
  # Infrastructure
  initial_s_vm_count: 12
  initial_m_vm_count: 6
  initial_l_vm_count: 3
  
  # Reward weights
  reward_wait_time_coef: 1.0
  reward_unutilization_coef: 0.5
  reward_cost_coef: 0.5
  reward_invalid_action_coef: 2.0
  
  max_episode_length: 620
  seed: 42
  save_experiment: true
  verbose: 1

# ===================================================================
# Experiment 8: Bursty Workload - Stress Test (200 tasks in bursts)
# ===================================================================
experiment_8:
  simulation_name: "Exp8_Bursty_StressTest"
  experiment_name: "exp8_bursty"
  experiment_type_dir: "Synthetic_Workloads"
  
  workload_mode: "CSV"
  cloudlet_trace_file: "traces/synthetic_bursty_400s.csv"  # 200 tasks, bursty
  
  mode: "train"
  algorithm: "MaskablePPO"
  timesteps: 50000
  learning_rate: 0.0001
  
  # Handle burst load
  initial_s_vm_count: 15  # More VMs for burst handling
  initial_m_vm_count: 8
  initial_l_vm_count: 4
  
  reward_wait_time_coef: 1.5  # Higher penalty for wait time in bursts
  reward_unutilization_coef: 0.4  # Lower during bursts (acceptable to over-provision)
  reward_cost_coef: 0.5
  reward_invalid_action_coef: 2.0
  
  max_episode_length: 420
  seed: 42
  save_experiment: true
  verbose: 1

# ===================================================================
# Experiment 5 Improved: Based on Experiment 4 Analysis
# ===================================================================
# Key improvements:
# 1. Reduced small/medium VMs, increased large VMs (better resource efficiency)
# 2. Increased utilization and cost penalties (address main weakness)
# 3. Increased learning rate and exploration (escape local optimum)
# 4. Longer training and episodes (more learning time)
# 5. Same workload as Exp4 for direct comparison
# ===================================================================

experiment_5_improved:
  # --- Identification ---
  simulation_name: "Exp5_CSV_ResourceOptimized"
  experiment_name: "exp5_resource_optimized"
  experiment_type_dir: "CSV_Optimized_V2"
  
  # --- Workload (Same as Exp4 for comparison) ---
  workload_mode: "CSV"
  cloudlet_trace_file: "traces/three_60max_8maxcores.csv"
  
  # --- Algorithm & Training ---
  mode: "train"
  algorithm: "MaskablePPO"
  timesteps: 100000              # 50000 → 100000 (2x longer training)
  learning_rate: 0.0003          # 0.0001 → 0.0003 (3x higher, escape local optimum)
  
  # --- PPO Hyperparameters ---
  n_steps: 512                   # Keep from Exp4
  batch_size: 128                # Keep from Exp4
  n_epochs: 15                   # Keep from Exp4
  gamma: 0.995                   # Keep from Exp4
  gae_lambda: 0.98               # Keep from Exp4
  clip_range: 0.2
  ent_coef: 0.05                 # 0.02 → 0.05 (more exploration)
  vf_coef: 0.5
  max_grad_norm: 0.5
  
  # --- VM Fleet (KEY IMPROVEMENT: More Large VMs, Fewer Small VMs) ---
  initial_s_vm_count: 5          # 10 → 5 (reduce idle resources)
  initial_m_vm_count: 3          # 5 → 3 (reduce idle resources)
  initial_l_vm_count: 4          # 2 → 4 (increase high-capacity VMs)
  
  # --- Reward Weights (KEY IMPROVEMENT: Penalize waste) ---
  reward_wait_time_coef: 0.8               # Slightly reduce (was dominating)
  reward_unutilization_coef: 1.0           # 0.5 → 1.0 (2x penalty for low utilization)
  reward_cost_coef: 0.8                    # NEW: Strong cost penalty
  reward_queue_penalty_coef: 0.55          # Keep from common
  reward_invalid_action_coef: 1.5          # Slightly increase
  
  # --- Episode Configuration ---
  max_episode_length: 200        # 100 → 200 (give more time for tasks to complete)
  
  # --- Infrastructure (Keep from Exp4) ---
  hosts_count: 32
  host_pes: 16
  host_pe_mips: 50000
  host_ram: 65536
  host_bw: 50000
  host_storage: 100000
  
  # --- VM Specs (Keep from common) ---
  small_vm_pes: 2
  small_vm_ram: 8192
  small_vm_bw: 1000
  small_vm_storage: 4000
  medium_vm_multiplier: 2
  large_vm_multiplier: 4
  
  # --- Other Settings ---
  seed: 42                       # Change seed for different initialization
  save_experiment: true
  verbose: 1                     # Show training progress
  log_interval: 1
  device: "auto"                 # Use GPU if available

