# Multi-DC Hierarchical MARL Training Monitoring

æœ¬æ–‡æ¡£è¯´æ˜å¦‚ä½•åœ¨è®­ç»ƒå¤šæ•°æ®ä¸­å¿ƒåˆ†å±‚ MARL ç³»ç»Ÿæ—¶ç›‘æ§è®­ç»ƒè¿›åº¦ã€æŸ¥çœ‹å¥–åŠ±å€¼å¹¶ä¿å­˜æœ€ä½³æ¨¡å‹ã€‚

## åŠŸèƒ½ç‰¹æ€§

1. **å®æ—¶è®­ç»ƒè¿›åº¦æ˜¾ç¤º**ï¼šæ˜¾ç¤ºæ¯ä¸ª episode çš„å¥–åŠ±å€¼
2. **åˆ†å±‚å¥–åŠ±è¿½è¸ª**ï¼šåˆ†åˆ«è¿½è¸ª Global Agent å’Œ Local Agent çš„å¥–åŠ±
3. **è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹**ï¼šå½“å¥–åŠ±å€¼æå‡æ—¶è‡ªåŠ¨ä¿å­˜æ¨¡å‹
4. **TensorBoard å¯è§†åŒ–**ï¼šå®æ—¶å¯è§†åŒ–è®­ç»ƒæ›²çº¿
5. **CSV æ—¥å¿—è®°å½•**ï¼šå°†è®­ç»ƒæ•°æ®ä¿å­˜ä¸º CSV æ–‡ä»¶ä»¥ä¾¿åç»­åˆ†æ

## æ–°å¢æ–‡ä»¶

### 1. Callback ç±»

`drl-manager/src/callbacks/save_on_best_reward_hierarchical.py`

- è¿½è¸ª Global å’Œ Local æ™ºèƒ½ä½“çš„å¥–åŠ±
- åœ¨å¥–åŠ±æå‡æ—¶è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹
- è®°å½•è®­ç»ƒè¿›åº¦åˆ° CSV
- è®°å½•æœ€ä½³ episode çš„è¯¦ç»†æ•°æ®

### 2. æ›´æ–°çš„è®­ç»ƒè„šæœ¬

`drl-manager/src/training/train_hierarchical_multidc_joint.py`

- é›†æˆäº† Monitor wrapper ç”¨äº episode ç»Ÿè®¡
- é›†æˆäº†æ–°çš„ hierarchical callback
- æ·»åŠ äº† TensorBoard æ—¥å¿—æ”¯æŒ
- æ·»åŠ äº†è¿›åº¦æ¡æ˜¾ç¤º

### 3. æ›´æ–°çš„ç¯å¢ƒ

`drl-manager/gym_cloudsimplus/envs/joint_training_env.py`

- åœ¨ `info` å­—å…¸ä¸­æ·»åŠ äº†å¥–åŠ±ä¿¡æ¯
- æ·»åŠ äº†èƒ½æºç»Ÿè®¡ä¿¡æ¯ï¼ˆgreen energy ratio, brown energy, wasted green energyï¼‰

## ä½¿ç”¨æ–¹æ³•

### 1. è¿è¡Œè®­ç»ƒ

```bash
cd drl-manager

# ä½¿ç”¨ alternating ç­–ç•¥ï¼ˆæ¨èï¼‰
python -m src.training.train_hierarchical_multidc_joint --config ../config.yml --experiment experiment_multi_dc_3 --strategy alternating --output_dir ../logs/multi_dc_training
```

### 2. è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¾“å‡º

è®­ç»ƒæ—¶ä½ ä¼šçœ‹åˆ°ç±»ä¼¼ä»¥ä¸‹çš„è¾“å‡ºï¼š

```
======================================================================
  Cycle 1/10
======================================================================
Training Global Agent...

Episode 1 completed (Timestep: 512)
  Episode Reward: 142.350
  Global Agent Reward: 85.120
  Local Agent Reward: 57.230
  Mean Reward (last 1 eps): 142.350
  Best Mean Reward: -inf
============================================================
ğŸ‰ New best mean reward! Saving models...
  âœ… Saved best global model to logs/multi_dc_training/best_global_model.zip
  âœ… Saved best local model to logs/multi_dc_training/best_local_model.zip
  âœ… Saved best episode details to logs/multi_dc_training/best_episode_details.csv
============================================================

Episode 2 completed (Timestep: 1024)
  Episode Reward: 158.750
  Global Agent Reward: 92.340
  Local Agent Reward: 66.410
  Mean Reward (last 2 eps): 150.550
  Best Mean Reward: 142.350
============================================================
ğŸ‰ New best mean reward! Saving models...
...
```

### 3. è¾“å‡ºæ–‡ä»¶ç»“æ„

è®­ç»ƒå®Œæˆåï¼Œè¾“å‡ºç›®å½•åŒ…å«ä»¥ä¸‹æ–‡ä»¶ï¼š

```
logs/multi_dc_training/
â”œâ”€â”€ monitor/                          # Monitor æ—¥å¿—
â”‚   â””â”€â”€ 0.monitor.csv                # Episode ç»Ÿè®¡
â”œâ”€â”€ tensorboard/                      # TensorBoard æ—¥å¿—
â”‚   â”œâ”€â”€ global/                      # Global Agent è®­ç»ƒæ›²çº¿
â”‚   â””â”€â”€ local/                       # Local Agent è®­ç»ƒæ›²çº¿
â”œâ”€â”€ checkpoints/                      # å®šæœŸæ£€æŸ¥ç‚¹
â”‚   â”œâ”€â”€ model_5000_steps.zip
â”‚   â”œâ”€â”€ model_10000_steps.zip
â”‚   â””â”€â”€ ...
â”œâ”€â”€ training_progress.csv            # è®­ç»ƒè¿›åº¦ CSV
â”œâ”€â”€ best_episode_details.csv         # æœ€ä½³ episode è¯¦ç»†æ•°æ®
â”œâ”€â”€ best_global_model.zip            # æœ€ä½³ Global æ¨¡å‹
â”œâ”€â”€ best_local_model.zip             # æœ€ä½³ Local æ¨¡å‹
â”œâ”€â”€ global_cycle_1.zip               # æ¯ä¸ª cycle çš„æ£€æŸ¥ç‚¹
â”œâ”€â”€ local_cycle_1.zip
â”œâ”€â”€ global_cycle_2.zip
â”œâ”€â”€ local_cycle_2.zip
â”œâ”€â”€ ...
â”œâ”€â”€ final_global_model.zip           # æœ€ç»ˆæ¨¡å‹
â””â”€â”€ final_local_model.zip
```

### 4. æŸ¥çœ‹ TensorBoard

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æˆ–è®­ç»ƒåï¼Œå¯ä»¥ä½¿ç”¨ TensorBoard æŸ¥çœ‹è®­ç»ƒæ›²çº¿ï¼š

```bash
# åœ¨æ–°ç»ˆç«¯ä¸­è¿è¡Œ
tensorboard --logdir=logs/multi_dc_training/tensorboard

# ç„¶ååœ¨æµè§ˆå™¨ä¸­è®¿é—®ï¼šhttp://localhost:6006
```

ä½ å¯ä»¥çœ‹åˆ°ï¼š
- **Global Agent**ï¼šå…¨å±€è·¯ç”±æ™ºèƒ½ä½“çš„å­¦ä¹ æ›²çº¿
- **Local Agent**ï¼šæœ¬åœ°è°ƒåº¦æ™ºèƒ½ä½“çš„å­¦ä¹ æ›²çº¿
- **Episode reward**ï¼šæ¯ä¸ª episode çš„æ€»å¥–åŠ±
- **Episode length**ï¼šæ¯ä¸ª episode çš„é•¿åº¦

### 5. åˆ†æè®­ç»ƒæ•°æ®

#### 5.1 ä½¿ç”¨ Pandas åˆ†æ CSV

```python
import pandas as pd
import matplotlib.pyplot as plt

# è¯»å–è®­ç»ƒè¿›åº¦
df = pd.read_csv("logs/multi_dc_training/training_progress.csv")

# ç»˜åˆ¶å¥–åŠ±æ›²çº¿
plt.figure(figsize=(12, 6))
plt.plot(df['timestep'], df['mean_reward'], label='Mean Reward')
plt.plot(df['timestep'], df['mean_global_reward'], label='Global Reward')
plt.plot(df['timestep'], df['mean_local_reward'], label='Local Reward')
plt.xlabel('Timesteps')
plt.ylabel('Reward')
plt.legend()
plt.title('Training Progress')
plt.grid(True)
plt.show()
```

#### 5.2 æŸ¥çœ‹æœ€ä½³ Episode

```python
# è¯»å–æœ€ä½³ episode è¯¦ç»†æ•°æ®
best_episode = pd.read_csv("logs/multi_dc_training/best_episode_details.csv")

print(f"Best episode had {len(best_episode)} steps")
print(f"Total reward: {best_episode['reward'].sum():.3f}")
print(f"Average global reward: {best_episode['global_reward'].mean():.3f}")
print(f"Average local reward: {best_episode['local_reward'].mean():.3f}")
```

### 6. åŠ è½½æœ€ä½³æ¨¡å‹

```python
from stable_baselines3 import PPO
from sb3_contrib import MaskablePPO

# åŠ è½½æœ€ä½³æ¨¡å‹
best_global_model = PPO.load("logs/multi_dc_training/best_global_model")
best_local_model = MaskablePPO.load("logs/multi_dc_training/best_local_model")

# ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†
# ...
```

## CSV æ–‡ä»¶æ ¼å¼

### training_progress.csv

| åˆ—å | æè¿° |
|------|------|
| `timestep` | å½“å‰è®­ç»ƒæ­¥æ•° |
| `episode` | Episode ç¼–å· |
| `episode_reward` | è¯¥ episode çš„æ€»å¥–åŠ± |
| `episode_global_reward` | è¯¥ episode çš„ Global Agent å¹³å‡å¥–åŠ± |
| `episode_local_reward` | è¯¥ episode çš„ Local Agent å¹³å‡å¥–åŠ± |
| `mean_reward` | æœ€è¿‘ 100 ä¸ª episodes çš„å¹³å‡æ€»å¥–åŠ± |
| `mean_global_reward` | æœ€è¿‘ 100 ä¸ª episodes çš„å¹³å‡ Global å¥–åŠ± |
| `mean_local_reward` | æœ€è¿‘ 100 ä¸ª episodes çš„å¹³å‡ Local å¥–åŠ± |
| `best_mean_reward` | åˆ°ç›®å‰ä¸ºæ­¢çš„æœ€ä½³å¹³å‡å¥–åŠ± |

### best_episode_details.csv

| åˆ—å | æè¿° |
|------|------|
| `timestep` | è®­ç»ƒæ­¥æ•° |
| `reward` | è¯¥æ­¥çš„æ€»å¥–åŠ± |
| `global_reward` | è¯¥æ­¥çš„ Global Agent å¥–åŠ± |
| `local_reward` | è¯¥æ­¥çš„ Local Agent å¹³å‡å¥–åŠ± |

### monitor/0.monitor.csv

Stable-Baselines3 Monitor ç”Ÿæˆçš„æ ‡å‡†æ ¼å¼ï¼š

| åˆ—å | æè¿° |
|------|------|
| `r` | Episode æ€»å¥–åŠ± |
| `l` | Episode é•¿åº¦ï¼ˆæ­¥æ•°ï¼‰|
| `t` | è®­ç»ƒæ—¶é—´ï¼ˆç§’ï¼‰|
| `global_reward` | Global Agent å¥–åŠ±ï¼ˆå¦‚æœåœ¨ info ä¸­ï¼‰|
| `local_reward` | Local Agent å¥–åŠ±ï¼ˆå¦‚æœåœ¨ info ä¸­ï¼‰|
| `total_reward` | æ€»å¥–åŠ±ï¼ˆå¦‚æœåœ¨ info ä¸­ï¼‰|
| `green_energy_ratio` | ç»¿è‰²èƒ½æºæ¯”ä¾‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰|
| `brown_energy_wh` | è¤è‰²èƒ½æºæ¶ˆè€—ï¼ˆå¦‚æœå¯ç”¨ï¼‰|
| `wasted_green_wh` | æµªè´¹çš„ç»¿è‰²èƒ½æºï¼ˆå¦‚æœå¯ç”¨ï¼‰|

## é…ç½®é€‰é¡¹

åœ¨ `config.yml` ä¸­çš„ `hierarchical_multi_dc.training` éƒ¨åˆ†ï¼š

```yaml
hierarchical_multi_dc:
  training:
    strategy: "alternating"           # "alternating" æˆ– "simultaneous"
    global_steps_per_cycle: 10000    # æ¯ä¸ª cycle è®­ç»ƒ Global Agent çš„æ­¥æ•°
    local_steps_per_cycle: 10000     # æ¯ä¸ª cycle è®­ç»ƒ Local Agent çš„æ­¥æ•°
    num_cycles: 10                    # æ€»å…±å¤šå°‘ä¸ªè®­ç»ƒ cycle
```

## æç¤ºå’ŒæŠ€å·§

### 1. è°ƒæ•´ Callback ä¿å­˜é¢‘ç‡

åœ¨ `train_hierarchical_multidc_joint.py` ä¸­ä¿®æ”¹ï¼š

```python
save_best_callback = SaveOnBestRewardHierarchicalCallback(
    log_dir=str(self.output_dir),
    global_model=self.global_model,
    local_model=self.local_model,
    save_freq=1000,  # ä¿®æ”¹æ­¤å€¼ä»¥æ”¹å˜æ£€æŸ¥é¢‘ç‡
    verbose=1
)
```

### 2. è°ƒæ•´æ£€æŸ¥ç‚¹ä¿å­˜é¢‘ç‡

```python
checkpoint_callback = CheckpointCallback(
    save_freq=5000,  # æ¯ 5000 æ­¥ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
    save_path=str(self.output_dir / "checkpoints"),
    name_prefix="model",
    verbose=1
)
```

### 3. ç›‘æ§ç‰¹å®šæŒ‡æ ‡

åœ¨ `_create_environment` ä¸­ä¿®æ”¹ `info_keywords` ä»¥è®°å½•æ›´å¤šæŒ‡æ ‡ï¼š

```python
info_keywords = (
    "global_reward", "local_reward", "total_reward",
    "cloudlets_routed", "cloudlets_completed",
    "green_energy_ratio", "brown_energy_wh", "wasted_green_wh",
    # æ·»åŠ æ›´å¤šä½ å…³å¿ƒçš„æŒ‡æ ‡
)
```

## æ•…éšœæ’é™¤

### é—®é¢˜ 1: çœ‹ä¸åˆ°è®­ç»ƒè¿›åº¦

**è§£å†³æ–¹æ¡ˆ**ï¼šç¡®ä¿ä½ çš„ç¯å¢ƒæ­£ç¡®è¿”å› `info` å­—å…¸ï¼Œå¹¶ä¸”åŒ…å«å¿…è¦çš„å¥–åŠ±ä¿¡æ¯ã€‚

### é—®é¢˜ 2: TensorBoard æ²¡æœ‰æ˜¾ç¤ºæ•°æ®

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. æ£€æŸ¥ `tensorboard_log` è·¯å¾„æ˜¯å¦æ­£ç¡®
2. ç¡®ä¿è®­ç»ƒå·²ç»è¿›è¡Œäº†è‡³å°‘ä¸€ä¸ª episode
3. åˆ·æ–° TensorBoard æµè§ˆå™¨é¡µé¢

### é—®é¢˜ 3: æ¨¡å‹æ²¡æœ‰ä¿å­˜

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. æ£€æŸ¥è¾“å‡ºç›®å½•æ˜¯å¦æœ‰å†™å…¥æƒé™
2. ç¡®è®¤å¥–åŠ±ç¡®å®åœ¨æå‡ï¼ˆåªæœ‰å½“å¥–åŠ±æå‡æ—¶æ‰ä¿å­˜ï¼‰
3. æŸ¥çœ‹æ—¥å¿—ä¸­æ˜¯å¦æœ‰é”™è¯¯ä¿¡æ¯

### é—®é¢˜ 4: CSV æ–‡ä»¶ä¸ºç©º

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. ç¡®ä¿è®­ç»ƒè‡³å°‘å®Œæˆäº†ä¸€ä¸ª episode
2. æ£€æŸ¥ Monitor wrapper æ˜¯å¦æ­£ç¡®åˆå§‹åŒ–
3. æŸ¥çœ‹æ˜¯å¦æœ‰æ–‡ä»¶å†™å…¥æƒé™é—®é¢˜

## æ€»ç»“

é€šè¿‡è¿™äº›æ”¹è¿›ï¼Œmulti-DC è®­ç»ƒç°åœ¨å…·æœ‰ä¸ single-DC è®­ç»ƒç›¸åŒçš„ç›‘æ§å’Œæ—¥å¿—åŠŸèƒ½ï¼š

âœ… å®æ—¶æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
âœ… åˆ†åˆ«è¿½è¸ª Global å’Œ Local Agent å¥–åŠ±
âœ… è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹
âœ… TensorBoard å¯è§†åŒ–
âœ… è¯¦ç»†çš„ CSV æ—¥å¿—
âœ… æœ€ä½³ episode è¯¦ç»†è®°å½•

è¿™äº›åŠŸèƒ½ä½¿å¾—è®­ç»ƒè¿‡ç¨‹æ›´åŠ é€æ˜ï¼Œä¾¿äºè°ƒè¯•å’Œä¼˜åŒ–ã€‚
