# TensorBoard æŒ‡æ ‡æŸ¥çœ‹æŒ‡å—

## ğŸ“Š æ¦‚è¿°

æ‚¨çš„å¤šæ•°æ®ä¸­å¿ƒå¼ºåŒ–å­¦ä¹ ç³»ç»Ÿç°åœ¨ä¼šè®°å½•è¯¦ç»†çš„è®­ç»ƒæŒ‡æ ‡ï¼ŒåŒ…æ‹¬ï¼š

1. **æ¯ä¸ª Agent çš„ Episode Reward**
   - Global Agent å¥–åŠ±
   - Local Agent å¥–åŠ±
   - æ€»å¥–åŠ±

2. **PPO ç¥ç»ç½‘ç»œæŸå¤±**
   - Policy Loss (ç­–ç•¥ç½‘ç»œæŸå¤±)
   - Value Loss (ä»·å€¼ç½‘ç»œæŸå¤±)
   - Entropy Loss (ç†µæŸå¤±)
   - KL Divergence (KL æ•£åº¦)
   - Clip Fraction (è£å‰ªæ¯”ä¾‹)

3. **è®­ç»ƒç»Ÿè®¡**
   - Episode é•¿åº¦
   - æ»šåŠ¨å¹³å‡å¥–åŠ±
   - å­¦ä¹ ç‡å˜åŒ–

---

## ğŸš€ å¯åŠ¨ TensorBoard

### 1. è®­ç»ƒå®Œæˆåï¼Œæ‰¾åˆ°æ—¥å¿—ç›®å½•

è®­ç»ƒæ—¥å¿—ä¿å­˜åœ¨ï¼š
```
logs/joint_training/<timestamp>/tensorboard/
â”œâ”€â”€ global/    # Global Agent çš„æ—¥å¿—
â””â”€â”€ local/     # Local Agent çš„æ—¥å¿—
```

### 2. å¯åŠ¨ TensorBoard

åœ¨ `drl-manager` ç›®å½•ä¸‹è¿è¡Œï¼š

```bash
# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
.venv\Scripts\activate  # Windows
# æˆ–
source .venv/bin/activate  # Linux/Mac

# å¯åŠ¨ TensorBoardï¼ŒæŒ‡å‘æ‚¨çš„å®éªŒç›®å½•
tensorboard --logdir=../logs/joint_training/<timestamp>/tensorboard

# æˆ–è€…æŸ¥çœ‹æ‰€æœ‰å®éªŒ
tensorboard --logdir=../logs/joint_training
```

### 3. åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€

TensorBoard ä¼šè¾“å‡ºä¸€ä¸ª URLï¼Œé€šå¸¸æ˜¯ï¼š
```
http://localhost:6006
```

åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€è¿™ä¸ªåœ°å€ã€‚

---

## ğŸ“ˆ æŸ¥çœ‹æŒ‡æ ‡è¯¦è§£

### **1. Episode Rewards (Episode å¥–åŠ±)**

åœ¨ TensorBoard å·¦ä¾§é€‰æ‹© **"SCALARS"** æ ‡ç­¾ã€‚

#### **å•ç‹¬çš„ Agent å¥–åŠ±**

æŸ¥æ‰¾ä»¥ä¸‹æŒ‡æ ‡ï¼š

```
global_agent/episode/reward          # Global Agent çš„å•ä¸ª episode æ€»å¥–åŠ±
global_agent/episode/mean_reward_100 # Global Agent æœ€è¿‘100ä¸ª episode çš„å¹³å‡å¥–åŠ±

local_agent/episode/reward           # Local Agent çš„å•ä¸ª episode æ€»å¥–åŠ±
local_agent/episode/mean_reward_100  # Local Agent æœ€è¿‘100ä¸ª episode çš„å¹³å‡å¥–åŠ±
```

**å¦‚ä½•å¯¹æ¯”ä¸¤ä¸ª Agentï¼Ÿ**
1. åœ¨å·¦ä¾§çš„æœç´¢æ¡†ä¸­è¾“å…¥ `episode/reward`
2. ä¼šæ˜¾ç¤º `global_agent` å’Œ `local_agent` çš„ä¸¤æ¡æ›²çº¿
3. å¯ä»¥ç”¨ä¸åŒé¢œè‰²åŒºåˆ†

#### **ç»„åˆå¥–åŠ±**

```
episode/reward              # æ€»çš„ episode å¥–åŠ± (global + local)
episode/global_reward       # Global Agent çš„å¹³å‡å•æ­¥å¥–åŠ±
episode/local_reward        # Local Agent çš„å¹³å‡å•æ­¥å¥–åŠ±
episode/mean_reward         # æ»šåŠ¨å¹³å‡æ€»å¥–åŠ± (100 episodes)
```

---

### **2. PPO æŸå¤±æ›²çº¿**

#### **Global Agent çš„æŸå¤±**

```
global/train/policy_loss        # ç­–ç•¥ç½‘ç»œæŸå¤±
global/train/value_loss         # ä»·å€¼ç½‘ç»œæŸå¤±
global/train/entropy_loss       # ç†µæŸå¤± (æ¢ç´¢ vs åˆ©ç”¨)
global/train/loss               # æ€»æŸå¤±
```

#### **Local Agent çš„æŸå¤±**

```
local/train/policy_loss         # ç­–ç•¥ç½‘ç»œæŸå¤±
local/train/value_loss          # ä»·å€¼ç½‘ç»œæŸå¤±
local/train/entropy_loss        # ç†µæŸå¤±
local/train/loss                # æ€»æŸå¤±
```

**è§£è¯»æŸå¤±æ›²çº¿ï¼š**
- **Policy Loss**: åº”è¯¥é€æ¸ä¸‹é™å¹¶ç¨³å®šï¼Œè¡¨ç¤ºç­–ç•¥åœ¨æ”¹è¿›
- **Value Loss**: åº”è¯¥ä¸‹é™ï¼Œè¡¨ç¤ºä»·å€¼å‡½æ•°ä¼°è®¡æ›´å‡†ç¡®
- **Entropy Loss**: å¦‚æœå¤ªä½ï¼Œå¯èƒ½è¿‡åº¦å¼€å‘ï¼ˆexploitationï¼‰ï¼›å¤ªé«˜åˆ™æ¢ç´¢è¿‡å¤š

---

### **3. PPO è®­ç»ƒæŒ‡æ ‡**

```
global/train/approx_kl          # KL æ•£åº¦ï¼ˆæ–°æ—§ç­–ç•¥å·®å¼‚ï¼‰
global/train/clip_fraction      # è¢«è£å‰ªçš„æ ·æœ¬æ¯”ä¾‹
global/train/explained_variance # ä»·å€¼å‡½æ•°çš„è§£é‡Šæ–¹å·®
global/train/learning_rate      # å­¦ä¹ ç‡ï¼ˆå¦‚æœä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦ï¼‰
global/train/n_updates          # æ›´æ–°æ¬¡æ•°

local/train/approx_kl
local/train/clip_fraction
local/train/explained_variance
local/train/learning_rate
local/train/n_updates
```

**å…³é”®æŒ‡æ ‡è§£è¯»ï¼š**
- **approx_kl**: åº”è¯¥ä¿æŒåœ¨ä¸€ä¸ªå°çš„èŒƒå›´ï¼ˆå¦‚ < 0.1ï¼‰ï¼Œè¿‡å¤§è¡¨ç¤ºç­–ç•¥å˜åŒ–å¤ªå¿«
- **clip_fraction**: PPO çš„æ ¸å¿ƒï¼Œé€šå¸¸åœ¨ 0.1-0.3 ä¹‹é—´æ˜¯å¥åº·çš„
- **explained_variance**: æ¥è¿‘ 1 è¡¨ç¤ºä»·å€¼å‡½æ•°å¾ˆå‡†ç¡®ï¼Œæ¥è¿‘ 0 è¡¨ç¤ºä¼°è®¡å¾ˆå·®

---

### **4. Rollout ç»Ÿè®¡**

```
global/rollout/mean_ep_reward   # Global rollout æœŸé—´çš„å¹³å‡ episode å¥–åŠ±
global/rollout/mean_ep_length   # Global rollout æœŸé—´çš„å¹³å‡ episode é•¿åº¦

local/rollout/mean_ep_reward
local/rollout/mean_ep_length
```

---

## ğŸ¨ TensorBoard é«˜çº§åŠŸèƒ½

### **å¯¹æ¯”å¤šä¸ªå®éªŒ**

å¦‚æœæ‚¨è¿è¡Œäº†å¤šæ¬¡è®­ç»ƒï¼š

```bash
tensorboard --logdir=../logs/joint_training
```

TensorBoard ä¼šè‡ªåŠ¨åŠ è½½æ‰€æœ‰å­ç›®å½•çš„å®éªŒï¼Œæ‚¨å¯ä»¥ï¼š
1. åœ¨å·¦ä¾§çœ‹åˆ°æ‰€æœ‰å®éªŒçš„åˆ—è¡¨
2. é€‰æ‹©/å–æ¶ˆé€‰æ‹©æŸäº›å®éªŒ
3. å¯¹æ¯”ä¸åŒé…ç½®çš„æ•ˆæœ

### **å¹³æ»‘æ›²çº¿**

å·¦ä¾§æœ‰ä¸€ä¸ª **"Smoothing"** æ»‘å—ï¼š
- æ‹–åŠ¨å®ƒå¯ä»¥å¹³æ»‘æ›²çº¿ï¼Œæ›´å®¹æ˜“çœ‹å‡ºè¶‹åŠ¿
- é»˜è®¤å€¼ 0.6 é€šå¸¸å°±å¾ˆå¥½

### **ä¸‹è½½æ•°æ®**

æ¯ä¸ªå›¾è¡¨å·¦ä¸‹è§’æœ‰ä¸‰ä¸ªæŒ‰é’®ï¼š
- ğŸ“¥ **Download**: ä¸‹è½½ CSV æ ¼å¼çš„æ•°æ®
- ğŸ” **Toggle Y-Axis**: åˆ‡æ¢ Y è½´åˆ»åº¦ï¼ˆçº¿æ€§/å¯¹æ•°ï¼‰
- ğŸ“Œ **Pin**: å›ºå®šå›¾è¡¨

---

## ğŸ“Š å…¸å‹çš„å¥åº·è®­ç»ƒæ›²çº¿

### **Episode Reward**
```
æ—¶é—´ â†’
  â†‘
å¥–|     â•±â€¾â€¾â€¾â€¾â€¾
åŠ±|   â•±
  | â•±
  |â•±___________
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ è®­ç»ƒæ­¥æ•°
```
- åˆæœŸä¸Šå‡
- ä¸­æœŸæ³¢åŠ¨ä½†æ•´ä½“å‘ä¸Š
- åæœŸè¶‹äºç¨³å®š

### **Policy Loss**
```
  â†‘
æŸ|â•²
å¤±|  â•²__
  |     â€¾â€¾â€¾â€¾â€¾â€¾
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ è®­ç»ƒæ­¥æ•°
```
- å¿«é€Ÿä¸‹é™
- ç„¶åè¶‹äºç¨³å®šï¼ˆå°å¹…æ³¢åŠ¨æ˜¯æ­£å¸¸çš„ï¼‰

### **Value Loss**
```
  â†‘
æŸ|â•²
å¤±|  â•²_
  |    â€¾â€¾â€¾â€¾â€¾
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ è®­ç»ƒæ­¥æ•°
```
- ç±»ä¼¼ Policy Loss
- å¯èƒ½ä¸‹é™æ›´å¹³æ»‘

---

## ğŸ” å¸¸è§é—®é¢˜è¯Šæ–­

### **1. Reward ä¸å¢é•¿æˆ–ä¸‹é™**

å¯èƒ½åŸå› ï¼š
- å­¦ä¹ ç‡å¤ªé«˜ â†’ æ£€æŸ¥ `train/learning_rate`
- KL æ•£åº¦å¤ªå¤§ â†’ æ£€æŸ¥ `train/approx_kl`
- ä»·å€¼å‡½æ•°ä¼°è®¡å·® â†’ æ£€æŸ¥ `train/explained_variance`

### **2. Loss éœ‡è¡å‰§çƒˆ**

å¯èƒ½åŸå› ï¼š
- å­¦ä¹ ç‡å¤ªé«˜ â†’ é™ä½ `learning_rate`
- Batch size å¤ªå° â†’ å¢å¤§ `batch_size`
- Clip range ä¸åˆé€‚ â†’ è°ƒæ•´ `clip_range`

### **3. Entropy å¿«é€Ÿé™è‡³ 0**

é—®é¢˜ï¼š
- Agent è¿‡æ—©æ”¶æ•›åˆ°æ¬¡ä¼˜ç­–ç•¥

è§£å†³ï¼š
- å¢å¤§ `ent_coef`ï¼ˆç†µç³»æ•°ï¼‰
- å¢åŠ æ¢ç´¢

---

## ğŸ’¾ å¯¼å‡ºè®­ç»ƒæ›²çº¿

### **æ–¹æ³• 1: ç›´æ¥åœ¨ TensorBoard ä¸‹è½½**
1. ç‚¹å‡»å›¾è¡¨å·¦ä¸‹è§’çš„ ğŸ“¥ æŒ‰é’®
2. ä¸‹è½½ CSV æ–‡ä»¶

### **æ–¹æ³• 2: ä½¿ç”¨ Python è„šæœ¬è¯»å–**

```python
from tensorboard.backend.event_processing import event_accumulator

# åŠ è½½ TensorBoard æ—¥å¿—
ea = event_accumulator.EventAccumulator('path/to/tensorboard/global')
ea.Reload()

# è·å–æŒ‡æ ‡
policy_loss = ea.Scalars('global/train/policy_loss')
rewards = ea.Scalars('global_agent/episode/reward')

# è½¬æ¢ä¸º DataFrame
import pandas as pd
df = pd.DataFrame(policy_loss)
```

### **æ–¹æ³• 3: ä½¿ç”¨ CSV æ—¥å¿—**

è®­ç»ƒè„šæœ¬å·²ç»è‡ªåŠ¨ä¿å­˜ CSVï¼š
```
logs/joint_training/<timestamp>/training_progress.csv
```

åŒ…å«ï¼š
- timestep
- episode
- episode_reward
- episode_global_reward
- episode_local_reward
- mean_reward
- best_mean_reward

ç›´æ¥ç”¨ Excel æˆ– Python æ‰“å¼€å³å¯ã€‚

---

## ğŸ“ ç¤ºä¾‹ï¼šå®Œæ•´æŸ¥çœ‹æµç¨‹

1. **å¯åŠ¨ TensorBoard**
   ```bash
   cd drl-manager
   .venv\Scripts\activate
   tensorboard --logdir=../logs/joint_training
   ```

2. **æ‰“å¼€æµè§ˆå™¨** â†’ `http://localhost:6006`

3. **æŸ¥çœ‹ Global Agent è¡¨ç°**
   - å·¦ä¾§æœç´¢ï¼š`global_agent/episode/reward`
   - è§‚å¯Ÿæ›²çº¿æ˜¯å¦ä¸Šå‡

4. **æŸ¥çœ‹ Local Agent è¡¨ç°**
   - å·¦ä¾§æœç´¢ï¼š`local_agent/episode/reward`
   - å¯¹æ¯”ä¸ Global Agent çš„å·®å¼‚

5. **æ£€æŸ¥æŸå¤±å‡½æ•°**
   - æœç´¢ï¼š`global/train/policy_loss`
   - æœç´¢ï¼š`global/train/value_loss`
   - ç¡®ä¿éƒ½åœ¨ä¸‹é™

6. **æ£€æŸ¥ PPO å¥åº·æŒ‡æ ‡**
   - `train/approx_kl` < 0.1 âœ“
   - `train/clip_fraction` åœ¨ 0.1-0.3 âœ“
   - `train/explained_variance` æ¥è¿‘ 1 âœ“

7. **è°ƒæ•´å¹³æ»‘åº¦**
   - æ‹–åŠ¨å·¦ä¾§ Smoothing æ»‘å—åˆ° 0.7-0.8
   - æ›´æ¸…æ™°åœ°çœ‹å‡ºè¶‹åŠ¿

---

## ğŸ¯ è®­ç»ƒæˆåŠŸçš„æ ‡å¿—

âœ… **Global Agent**
- Episode reward ç¨³å®šä¸Šå‡
- Policy loss ä¸‹é™å¹¶è¶‹äºç¨³å®š
- Value loss ä¸‹é™
- Explained variance > 0.5

âœ… **Local Agent**
- Episode reward ä¸Šå‡
- Policy loss ä¸‹é™
- Clip fraction åœ¨ 0.1-0.3

âœ… **æ•´ä½“**
- æ€» reward = global_reward + local_reward æŒç»­æ”¹è¿›
- æ²¡æœ‰çªç„¶çš„å´©æºƒæˆ–éœ‡è¡
- KL æ•£åº¦ä¿æŒåœ¨åˆç†èŒƒå›´

---

## ğŸ› ï¸ è¿›é˜¶æŠ€å·§

### **1. å®æ—¶ç›‘æ§è®­ç»ƒ**

è®­ç»ƒæ—¶åŒæ—¶æ‰“å¼€ TensorBoardï¼š
```bash
# Terminal 1: è¿è¡Œè®­ç»ƒ
python entrypoint_multidc.py

# Terminal 2: å¯åŠ¨ TensorBoard
tensorboard --logdir=../logs/joint_training
```

TensorBoard ä¼šè‡ªåŠ¨åˆ·æ–°ï¼Œå®æ—¶æ˜¾ç¤ºæœ€æ–°æ•°æ®ã€‚

### **2. å¯¹æ¯”ä¸åŒé…ç½®**

ä¿®æ”¹ `config.yml` ä¸­çš„å‚æ•°åé‡æ–°è®­ç»ƒï¼ŒTensorBoard ä¼šæ˜¾ç¤ºæ‰€æœ‰å®éªŒï¼š
- ä¸åŒå­¦ä¹ ç‡
- ä¸åŒ batch size
- ä¸åŒå¥–åŠ±æƒé‡

### **3. ä½¿ç”¨ TensorBoard.dev åˆ†äº«**

å°†æ‚¨çš„å®éªŒä¸Šä¼ åˆ°äº‘ç«¯åˆ†äº«ï¼š
```bash
tensorboard dev upload --logdir ../logs/joint_training/<timestamp>/tensorboard
```

ä¼šç”Ÿæˆä¸€ä¸ªå…¬å¼€é“¾æ¥ï¼Œå¯ä»¥åˆ†äº«ç»™ä»–äººã€‚

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [Stable-Baselines3 TensorBoard Integration](https://stable-baselines3.readthedocs.io/en/master/guide/tensorboard.html)
- [TensorBoard å®˜æ–¹æŒ‡å—](https://www.tensorflow.org/tensorboard/get_started)
- [PPO ç®—æ³•è¯¦è§£](https://spinningup.openai.com/en/latest/algorithms/ppo.html)

---

## ğŸ†˜ éœ€è¦å¸®åŠ©ï¼Ÿ

å¦‚æœæ‚¨åœ¨æŸ¥çœ‹ TensorBoard æ—¶é‡åˆ°é—®é¢˜ï¼š
1. æ£€æŸ¥æ—¥å¿—ç›®å½•æ˜¯å¦å­˜åœ¨
2. ç¡®ä¿è®­ç»ƒè„šæœ¬æ­£ç¡®è¿è¡Œ
3. æŸ¥çœ‹ `training_progress.csv` å¤‡ç”¨æ•°æ®
4. æ£€æŸ¥é˜²ç«å¢™æ˜¯å¦é˜»æ­¢äº†ç«¯å£ 6006

---

**Happy Training! ğŸš€**

