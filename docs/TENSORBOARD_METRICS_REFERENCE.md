# TensorBoard æŒ‡æ ‡å¿«é€Ÿå‚è€ƒ

## ğŸ“‹ æ‰€æœ‰å¯ç”¨æŒ‡æ ‡åˆ—è¡¨

### ğŸ¯ Episode å¥–åŠ±æŒ‡æ ‡

| æŒ‡æ ‡åç§° | ç±»å‹ | è¯´æ˜ | æœŸæœ›è¶‹åŠ¿ |
|---------|------|------|---------|
| `global_agent/episode/reward` | æ ‡é‡ | Global Agent å•ä¸ª episode çš„æ€»å¥–åŠ± | â†— ä¸Šå‡ |
| `global_agent/episode/mean_reward_100` | æ ‡é‡ | Global Agent æœ€è¿‘100ä¸ª episode çš„å¹³å‡å¥–åŠ± | â†— ä¸Šå‡å¹¶ç¨³å®š |
| `global_agent/episode/length` | æ ‡é‡ | Global Agent episode é•¿åº¦ï¼ˆæ­¥æ•°ï¼‰ | â†’ ç¨³å®š |
| `local_agent/episode/reward` | æ ‡é‡ | Local Agent å•ä¸ª episode çš„æ€»å¥–åŠ± | â†— ä¸Šå‡ |
| `local_agent/episode/mean_reward_100` | æ ‡é‡ | Local Agent æœ€è¿‘100ä¸ª episode çš„å¹³å‡å¥–åŠ± | â†— ä¸Šå‡å¹¶ç¨³å®š |
| `local_agent/episode/length` | æ ‡é‡ | Local Agent episode é•¿åº¦ï¼ˆæ­¥æ•°ï¼‰ | â†’ ç¨³å®š |
| `episode/reward` | æ ‡é‡ | æ€» episode å¥–åŠ± (global + local) | â†— ä¸Šå‡ |
| `episode/global_reward` | æ ‡é‡ | Global Agent å¹³å‡å•æ­¥å¥–åŠ± | â†— ä¸Šå‡ |
| `episode/local_reward` | æ ‡é‡ | Local Agent å¹³å‡å•æ­¥å¥–åŠ± | â†— ä¸Šå‡ |
| `episode/mean_reward` | æ ‡é‡ | æ»šåŠ¨å¹³å‡æ€»å¥–åŠ± (100 episodes) | â†— ä¸Šå‡å¹¶ç¨³å®š |
| `episode/mean_global_reward` | æ ‡é‡ | Global æ»šåŠ¨å¹³å‡å¥–åŠ± | â†— ä¸Šå‡å¹¶ç¨³å®š |
| `episode/mean_local_reward` | æ ‡é‡ | Local æ»šåŠ¨å¹³å‡å¥–åŠ± | â†— ä¸Šå‡å¹¶ç¨³å®š |

---

### ğŸ§  PPO ç¥ç»ç½‘ç»œæŸå¤±

| æŒ‡æ ‡åç§° | Agent | è¯´æ˜ | æœŸæœ›è¶‹åŠ¿ | å¥åº·èŒƒå›´ |
|---------|-------|------|---------|---------|
| `global/train/policy_loss` | Global | ç­–ç•¥ç½‘ç»œæŸå¤± | â†˜ ä¸‹é™å¹¶ç¨³å®š | å°å¹…æ³¢åŠ¨ |
| `global/train/value_loss` | Global | ä»·å€¼ç½‘ç»œæŸå¤± | â†˜ ä¸‹é™å¹¶ç¨³å®š | æ¥è¿‘ 0 |
| `global/train/entropy_loss` | Global | ç†µæŸå¤± | â†˜ ç¼“æ…¢ä¸‹é™ | > 0 (ä¿æŒæ¢ç´¢) |
| `global/train/loss` | Global | æ€»æŸå¤± | â†˜ ä¸‹é™ | - |
| `local/train/policy_loss` | Local | ç­–ç•¥ç½‘ç»œæŸå¤± | â†˜ ä¸‹é™å¹¶ç¨³å®š | å°å¹…æ³¢åŠ¨ |
| `local/train/value_loss` | Local | ä»·å€¼ç½‘ç»œæŸå¤± | â†˜ ä¸‹é™å¹¶ç¨³å®š | æ¥è¿‘ 0 |
| `local/train/entropy_loss` | Local | ç†µæŸå¤± | â†˜ ç¼“æ…¢ä¸‹é™ | > 0 (ä¿æŒæ¢ç´¢) |
| `local/train/loss` | Local | æ€»æŸå¤± | â†˜ ä¸‹é™ | - |

---

### ğŸ“Š PPO è®­ç»ƒè´¨é‡æŒ‡æ ‡

| æŒ‡æ ‡åç§° | Agent | è¯´æ˜ | å¥åº·èŒƒå›´ | è­¦å‘Šä¿¡å· |
|---------|-------|------|---------|---------|
| `global/train/approx_kl` | Global | KL æ•£åº¦ï¼ˆç­–ç•¥å˜åŒ–å¹…åº¦ï¼‰ | < 0.1 | > 0.2 (å˜åŒ–å¤ªå¿«) |
| `global/train/clip_fraction` | Global | è¢«è£å‰ªçš„æ ·æœ¬æ¯”ä¾‹ | 0.1 - 0.3 | > 0.5 æˆ– < 0.05 |
| `global/train/explained_variance` | Global | ä»·å€¼å‡½æ•°è§£é‡Šæ–¹å·® | > 0.5 | < 0 (ä¼°è®¡å¾ˆå·®) |
| `global/train/learning_rate` | Global | å½“å‰å­¦ä¹ ç‡ | é€’å‡ (å¦‚æœä½¿ç”¨è°ƒåº¦) | - |
| `global/train/n_updates` | Global | ç´¯è®¡æ›´æ–°æ¬¡æ•° | é€’å¢ | - |
| `local/train/approx_kl` | Local | KL æ•£åº¦ | < 0.1 | > 0.2 |
| `local/train/clip_fraction` | Local | è£å‰ªæ¯”ä¾‹ | 0.1 - 0.3 | > 0.5 æˆ– < 0.05 |
| `local/train/explained_variance` | Local | è§£é‡Šæ–¹å·® | > 0.5 | < 0 |
| `local/train/learning_rate` | Local | å­¦ä¹ ç‡ | é€’å‡ | - |
| `local/train/n_updates` | Local | æ›´æ–°æ¬¡æ•° | é€’å¢ | - |

---

### ğŸ² Rollout ç»Ÿè®¡

| æŒ‡æ ‡åç§° | Agent | è¯´æ˜ | ç”¨é€” |
|---------|-------|------|-----|
| `global/rollout/mean_ep_reward` | Global | Rollout æœŸé—´å¹³å‡ episode å¥–åŠ± | ç›‘æ§é‡‡æ ·è´¨é‡ |
| `global/rollout/mean_ep_length` | Global | Rollout æœŸé—´å¹³å‡ episode é•¿åº¦ | ç›‘æ§ episode ç¨³å®šæ€§ |
| `local/rollout/mean_ep_reward` | Local | Rollout æœŸé—´å¹³å‡ episode å¥–åŠ± | ç›‘æ§é‡‡æ ·è´¨é‡ |
| `local/rollout/mean_ep_length` | Local | Rollout æœŸé—´å¹³å‡ episode é•¿åº¦ | ç›‘æ§ episode ç¨³å®šæ€§ |

---

## ğŸš¨ å¼‚å¸¸æ¨¡å¼è¯†åˆ«

### âŒ **é—®é¢˜ 1: Reward ä¸å¢é•¿**

**ç—‡çŠ¶ï¼š**
- `episode/reward` æ›²çº¿å¹³å¦æˆ–ä¸‹é™
- `episode/mean_reward` æ²¡æœ‰æ”¹å–„

**è¯Šæ–­æŒ‡æ ‡ï¼š**
```
global/train/approx_kl > 0.2        # KL æ•£åº¦è¿‡å¤§
global/train/explained_variance < 0  # ä»·å€¼ä¼°è®¡å¾ˆå·®
global/train/policy_loss å‰§çƒˆéœ‡è¡    # ç­–ç•¥ä¸ç¨³å®š
```

**å¯èƒ½åŸå› ï¼š**
- å­¦ä¹ ç‡å¤ªé«˜
- å¥–åŠ±å‡½æ•°è®¾è®¡é—®é¢˜
- ç¯å¢ƒéšæœºæ€§å¤ªå¤§

**è§£å†³æ–¹æ¡ˆï¼š**
1. é™ä½å­¦ä¹ ç‡ (`learning_rate: 0.0001`)
2. å¢å¤§ `n_steps` (æ›´å¤šç»éªŒ)
3. å¢å¤§ `batch_size` (æ›´ç¨³å®šæ›´æ–°)

---

### âŒ **é—®é¢˜ 2: Loss éœ‡è¡å‰§çƒˆ**

**ç—‡çŠ¶ï¼š**
- `train/policy_loss` ä¸Šä¸‹å‰§çƒˆæ³¢åŠ¨
- `train/value_loss` ä¸ç¨³å®š

**è¯Šæ–­æŒ‡æ ‡ï¼š**
```
train/approx_kl æ³¢åŠ¨å¾ˆå¤§            # ç­–ç•¥å˜åŒ–ä¸ç¨³å®š
train/clip_fraction > 0.5          # è¿‡å¤šæ ·æœ¬è¢«è£å‰ª
```

**å¯èƒ½åŸå› ï¼š**
- å­¦ä¹ ç‡å¤ªé«˜
- Batch size å¤ªå°
- Clip range ä¸åˆé€‚

**è§£å†³æ–¹æ¡ˆï¼š**
1. é™ä½ `learning_rate`
2. å¢å¤§ `batch_size`
3. è°ƒæ•´ `clip_range` (é»˜è®¤ 0.2)
4. å‡å°‘ `n_epochs` (æ¯æ¬¡æ›´æ–°çš„è®­ç»ƒè½®æ•°)

---

### âŒ **é—®é¢˜ 3: è¿‡æ—©æ”¶æ•›ï¼ˆPremature Convergenceï¼‰**

**ç—‡çŠ¶ï¼š**
- `train/entropy_loss` å¿«é€Ÿé™è‡³æ¥è¿‘ 0
- Reward åœ¨æ¬¡ä¼˜æ°´å¹³åœæ»

**è¯Šæ–­æŒ‡æ ‡ï¼š**
```
train/entropy_loss â‰ˆ 0              # æ²¡æœ‰æ¢ç´¢
train/clip_fraction < 0.05          # ç­–ç•¥å‡ ä¹ä¸æ›´æ–°
episode/reward åœæ»åœ¨æ¬¡ä¼˜å€¼         # é™·å…¥å±€éƒ¨æœ€ä¼˜
```

**å¯èƒ½åŸå› ï¼š**
- ç†µç³»æ•°å¤ªå°
- æ¢ç´¢ä¸è¶³

**è§£å†³æ–¹æ¡ˆï¼š**
1. å¢å¤§ `ent_coef` (ç†µç³»æ•°ï¼Œé»˜è®¤ 0.01 â†’ 0.05)
2. ä½¿ç”¨ exploration noise
3. å¢åŠ è®­ç»ƒæ—¶é•¿

---

### âŒ **é—®é¢˜ 4: ä»·å€¼å‡½æ•°ä¼°è®¡å·®**

**ç—‡çŠ¶ï¼š**
- `train/explained_variance` æŒç»­ä¸ºè´Ÿæˆ–æ¥è¿‘ 0
- Reward æå‡ç¼“æ…¢

**è¯Šæ–­æŒ‡æ ‡ï¼š**
```
train/explained_variance < 0        # ä»·å€¼ä¼°è®¡æ¯”å¹³å‡å€¼è¿˜å·®
train/value_loss ä¸ä¸‹é™            # ä»·å€¼ç½‘ç»œå­¦ä¸åˆ°ä¸œè¥¿
```

**å¯èƒ½åŸå› ï¼š**
- ä»·å€¼ç½‘ç»œå®¹é‡ä¸è¶³
- Reward ä¿¡å·å¤ªç¨€ç–
- Gamma è®¾ç½®ä¸å½“

**è§£å†³æ–¹æ¡ˆï¼š**
1. å¢å¤§ä»·å€¼ç½‘ç»œè§„æ¨¡
2. è°ƒæ•´ `gamma` (æŠ˜æ‰£å› å­)
3. æ”¹å–„ reward shaping
4. å¢å¤§ `n_steps` (æ›´é•¿çš„è½¨è¿¹)

---

### âŒ **é—®é¢˜ 5: Global å’Œ Local Agents ä¸åè°ƒ**

**ç—‡çŠ¶ï¼š**
- `global_agent/episode/reward` ä¸Šå‡
- `local_agent/episode/reward` ä¸‹é™ï¼ˆæˆ–åä¹‹ï¼‰
- æ€» `episode/reward` æ”¹å–„ç¼“æ…¢

**è¯Šæ–­æŒ‡æ ‡ï¼š**
```
global_agent/episode/reward â†—      # Global æ”¹è¿›
local_agent/episode/reward â†˜       # Local é€€åŒ–
episode/reward éœ‡è¡                # æ€»ä½“ä¸ç¨³å®š
```

**å¯èƒ½åŸå› ï¼š**
- äº¤æ›¿è®­ç»ƒå‘¨æœŸä¸å¹³è¡¡
- å¥–åŠ±å‡½æ•°å†²çª
- å­¦ä¹ ç‡ä¸åŒ¹é…

**è§£å†³æ–¹æ¡ˆï¼š**
1. è°ƒæ•´ `global_steps_per_cycle` å’Œ `local_steps_per_cycle` æ¯”ä¾‹
2. é‡æ–°è®¾è®¡å¥–åŠ±æƒé‡ï¼Œç¡®ä¿åä½œ
3. ä½¿ç”¨ç›¸ä¼¼çš„å­¦ä¹ ç‡
4. è€ƒè™‘åˆ‡æ¢åˆ° simultaneous training

---

## ğŸ“ˆ å¥åº·è®­ç»ƒçš„æŒ‡æ ‡ç‰¹å¾

### âœ… **ç†æƒ³æ›²çº¿æ¨¡å¼**

#### **Episode Reward**
```
â†‘
â”‚         â•±â€¾â€¾â€¾â€¾â€¾â€¾
â”‚       â•±
â”‚     â•±
â”‚   â•±
â”‚ â•±
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ timesteps
```
- åˆæœŸå¿«é€Ÿä¸Šå‡
- ä¸­æœŸç¨³æ­¥æ”¹è¿›
- åæœŸè¶‹äºç¨³å®šï¼ˆå¯èƒ½æœ‰å°å¹…æ³¢åŠ¨ï¼‰

#### **Policy Loss**
```
â†‘
â”‚â•²
â”‚ â•²___
â”‚     â€¾â€¾â€¾â€¾â€¾â€¾
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ timesteps
```
- å¿«é€Ÿä¸‹é™
- ç¨³å®šåœ¨ä¸€ä¸ªä½æ°´å¹³
- å…è®¸å°å¹…æ³¢åŠ¨

#### **Value Loss**
```
â†‘
â”‚â•²
â”‚ â•²__
â”‚    â€¾â€¾â€¾â€¾â€¾
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ timesteps
```
- æŒç»­ä¸‹é™
- è¶‹è¿‘äº 0
- æ¯” policy loss æ›´å¹³æ»‘

#### **Entropy**
```
â†‘
â”‚â•²
â”‚ â•²
â”‚  â•²__
â”‚     â€¾â€¾â€¾â€¾
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ timesteps
```
- ç¼“æ…¢ä¸‹é™
- ä¿æŒåœ¨ > 0 çš„æ°´å¹³ï¼ˆä¿æŒæ¢ç´¢ï¼‰

#### **Explained Variance**
```
â†‘ 1.0 â”¼        â•±â€¾â€¾
â”‚       â•±
â”‚     â•±
â”‚ 0.5 â”¤  â•±
â”‚   â•±
â”‚ 0.0 â”¼
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ timesteps
```
- å¿«é€Ÿä¸Šå‡
- ç¨³å®šåœ¨ > 0.5 çš„é«˜æ°´å¹³

---

## ğŸ¯ å…³é”®æŒ‡æ ‡æ£€æŸ¥æ¸…å•

è®­ç»ƒå®Œæˆåï¼ŒæŒ‰ä»¥ä¸‹æ¸…å•æ£€æŸ¥ï¼š

### Global Agent
- [ ] `episode/reward` æŒç»­ä¸Šå‡ âœ“
- [ ] `train/policy_loss` ä¸‹é™å¹¶ç¨³å®š âœ“
- [ ] `train/value_loss` < 10 âœ“
- [ ] `train/explained_variance` > 0.5 âœ“
- [ ] `train/approx_kl` < 0.1 âœ“
- [ ] `train/clip_fraction` åœ¨ 0.1-0.3 âœ“
- [ ] `train/entropy_loss` > 0 (æœ‰æ¢ç´¢) âœ“

### Local Agent
- [ ] `episode/reward` æŒç»­ä¸Šå‡ âœ“
- [ ] `train/policy_loss` ä¸‹é™å¹¶ç¨³å®š âœ“
- [ ] `train/value_loss` < 10 âœ“
- [ ] `train/explained_variance` > 0.5 âœ“
- [ ] `train/approx_kl` < 0.1 âœ“
- [ ] `train/clip_fraction` åœ¨ 0.1-0.3 âœ“
- [ ] `train/entropy_loss` > 0 âœ“

### åä½œæ€§
- [ ] `episode/reward` = global + local ååŒæ”¹è¿› âœ“
- [ ] ä¸¤ä¸ª agent çš„ reward éƒ½åœ¨å¢é•¿ âœ“
- [ ] æ²¡æœ‰ä¸€ä¸ª agent ä¸¥é‡æ‹–åè…¿ âœ“

---

## ğŸ’¡ æŸ¥çœ‹æŠ€å·§

### 1. **ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿‡æ»¤**
åœ¨ TensorBoard æœç´¢æ¡†ä¸­ï¼š
- `.*policy_loss` â†’ æ˜¾ç¤ºæ‰€æœ‰ policy loss
- `global/.*` â†’ æ˜¾ç¤ºæ‰€æœ‰ global æŒ‡æ ‡
- `.*episode/reward` â†’ æ˜¾ç¤ºæ‰€æœ‰ episode reward

### 2. **è°ƒæ•´æ—¶é—´è½´**
- X è½´å¯é€‰æ‹©ï¼š`Step`, `Relative`, `Wall Time`
- å¯¹æ¯”å®éªŒæ—¶ä½¿ç”¨ `Step`

### 3. **ä½¿ç”¨ Tag è¿‡æ»¤å™¨**
- å·¦ä¾§å¯ä»¥æŒ‰ tag åˆ†ç»„
- æŠ˜å ä¸éœ€è¦çš„åˆ†ç»„

### 4. **å¯¼å‡ºå›¾ç‰‡**
- ç‚¹å‡»å›¾è¡¨å³ä¸Šè§’çš„ä¸‰ä¸ªç‚¹
- é€‰æ‹© "Download as PNG" æˆ– "Download as SVG"

---

## ğŸ“ CSV æ—¥å¿—å­—æ®µè¯´æ˜

`training_progress.csv` åŒ…å«ä»¥ä¸‹å­—æ®µï¼š

| å­—æ®µ | è¯´æ˜ |
|-----|-----|
| `timestep` | å½“å‰è®­ç»ƒæ­¥æ•° |
| `episode` | Episode ç¼–å· |
| `episode_reward` | å½“å‰ episode çš„æ€»å¥–åŠ± |
| `episode_global_reward` | Global Agent çš„å¹³å‡å•æ­¥å¥–åŠ± |
| `episode_local_reward` | Local Agent çš„å¹³å‡å•æ­¥å¥–åŠ± |
| `mean_reward` | æœ€è¿‘100ä¸ª episode çš„å¹³å‡æ€»å¥–åŠ± |
| `mean_global_reward` | æœ€è¿‘100ä¸ª episode çš„å¹³å‡ global å¥–åŠ± |
| `mean_local_reward` | æœ€è¿‘100ä¸ª episode çš„å¹³å‡ local å¥–åŠ± |
| `best_mean_reward` | è¿„ä»Šä¸ºæ­¢çš„æœ€ä½³å¹³å‡å¥–åŠ± |

---

**ç¥è®­ç»ƒé¡ºåˆ©ï¼æœ‰ä»»ä½•é—®é¢˜éšæ—¶æŸ¥é˜…æ­¤æ–‡æ¡£ã€‚** ğŸš€

